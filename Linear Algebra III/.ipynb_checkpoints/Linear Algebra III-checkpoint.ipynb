{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a0bdbd",
   "metadata": {},
   "source": [
    "## Linear Algebra Part III\n",
    "\n",
    "In this final article of the three part series, we will talk about two types of decomposition, specifically LU and Eigen Decomposition. And then finally talk about how we can use Linear Algebra to solve the Linear Regression Problem. \n",
    "\n",
    "If you havent read the first two articles, they are linked below:\n",
    "1. Linear Algebra with NumPy- Part I\n",
    "2. Linear Algebra with NumPy- Part II \n",
    "\n",
    "For this final article, we will combine NumPy and SciPy to perform some decompositions. \n",
    "\n",
    "### Part I: Introduction \n",
    "Before jumping into the two decomposition, lets talk a little bit about what matrix decompositions are and why they are helpful. \n",
    "\n",
    "A matrix decomposition or matrix factorization is essentially reducing your matrix to a product of matrices. The decomposition is done because it makes performing computations easier on the resulting matrices than the original matrix. \n",
    "\n",
    "A common analogy for matrix decomposition is the factoring of numbers, such as the factoring of 10 into 2 x 5. For this reason, matrix decomposition is also called matrix factorization. Like factoring real values, there are many ways to decompose a matrix, hence there are a range of different matrix decomposition techniques, each helping solve a particular class of problems. Some examples include LU decomposition, Eigen decomposition, QR decomposition, & Singular Value decomposition. <sup>1</sup> \n",
    "\n",
    "\n",
    "### Part II: LU Decomposition\n",
    "\n",
    "The first type of decomposition we will look at is the LU decomposition. Here the \"L\" stands for Lower Triangular Matrix and \"U\" stands for Upper Triangular Matrix. This means we decompose our original matrix into the product of a lower and upper triangular matrices. This type of decompoisition is particularly useful when solving a linear system of equations; because it is much easier to deal with triangular matrices, it is very desirable to decompose matrices into product of triangular matrices when solving linear equations.\n",
    "\n",
    "We can write this as: \n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "where $A$ is a matrix, $L$ is the lower triangular matrix and $U$ is the upper triangular matrix. \n",
    "\n",
    "In this article, we will talk about three different ways to decompose a matrix into a lower and upper triangular matrices, and they differ in the way they set up the Lower or Upper triangular matrix:\n",
    "1. Doolittle ($diag(L) = 1$)\n",
    "2. Crout ($diag(U) = 1$)\n",
    "3. Cholesky ($diag(U) = diag(L)$)\n",
    "\n",
    "We will dive into each one and show how different set ups give us the same result. \n",
    "\n",
    "1. <b> Doolittle </b>\n",
    "\n",
    "In this method of LU decomposition, the Lower Matrix is set up as:\n",
    "\n",
    "$L_{3x3} = \\begin{pmatrix} \n",
    "        1 & 0 & 0  \\\\\n",
    "        x & 1 & 0  \\\\\n",
    "        x & x & 1 \\\\\n",
    "     \\end{pmatrix}$\n",
    "\n",
    "where x is any number. \n",
    "\n",
    "Let A be a 3x3 matrix \n",
    "\n",
    "$A = \\begin{pmatrix} \n",
    "        1 & 2 & 4  \\\\\n",
    "        3 & 8 & 14 \\\\\n",
    "        2 & 6 & 13  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "We can set the $L$ and $U$ matrix as:\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        1 & 0 & 0  \\\\\n",
    "        L_{2,1} & 1 & 0 \\\\\n",
    "        L_{3,1} & L_{3,2} & 1  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "$U = \\begin{pmatrix} \n",
    "        U_{1,1} & U_{1,2} & U_{1,3}  \\\\\n",
    "        0 & U_{2,2} & U_{2,3} \\\\\n",
    "        0 & 0 & U_{3,3}  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "We can caluculate the product LU as:\n",
    "\n",
    "$LU = \\begin{pmatrix} \n",
    "        U_{1,1} & U_{1,2} & U_{1,3}  \\\\\n",
    "        L_{2,1}U_{1,1} & L_{2,1}U_{1,2} + U_{2,2} & L_{2,1}U_{1,3} + U_{2,3} \\\\\n",
    "        L_{3,1}U_{1,1} & L_{3,1}U_{1,2} + L_{3,2}U_{2,2} & L_{3,1}U_{1,3} + L_{3,2}U_{2,3} + U_{3,3}  \n",
    "     \\end{pmatrix}=\n",
    "     \\begin{pmatrix}\n",
    "        1 & 2 & 4  \\\\\n",
    "        3 & 8 & 14 \\\\\n",
    "        2 & 6 & 13  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "Once you solve the above equation we get the following values:\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        1 & 0 & 0  \\\\\n",
    "        3 & 1 & 0 \\\\\n",
    "        2 & 1 & 1  \n",
    "     \\end{pmatrix}$\n",
    "$U = \\begin{pmatrix} \n",
    "        1 & 2 & 4  \\\\\n",
    "        0 & 2 & 2 \\\\\n",
    "        0 & 0 & 3  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "We can confirm, multiplying these two matrices would give us the original matrix A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cc36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef0addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([[1,0,0],[3,1,0],[2,1,1]])\n",
    "u = np.array([[1,2,4], [0,2,2],[0,0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b498e04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [ 3,  8, 14],\n",
       "       [ 2,  6, 13]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(l,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c02a1",
   "metadata": {},
   "source": [
    "As you can see, the product produces the same matrix A. \n",
    "\n",
    "The second method is <b>Crout</b>. In this method the upper triangular matrix has 1's on its diagonal. \n",
    "\n",
    "Taking the same matrix A as an example:\n",
    "\n",
    "$A = \\begin{pmatrix} \n",
    "        1 & 2 & 4  \\\\\n",
    "        3 & 8 & 14 \\\\\n",
    "        2 & 6 & 13  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "We can set the $L$ and $U$ matrix as:\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        L_{1,1} & 0 & 0  \\\\\n",
    "        L_{2,1} & L_{2,2} & 0 \\\\\n",
    "        L_{3,1} & L_{3,2} & L_{3,3}  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "$U = \\begin{pmatrix} \n",
    "        1 & U_{1,2} & U_{1,3}  \\\\\n",
    "        0 & 1 & U_{2,3} \\\\\n",
    "        0 & 0 & 1  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "The product of the two matrices would be:\n",
    "\n",
    "$LU = \\begin{pmatrix} \n",
    "        L_{1,1} & L_{1,1}U_{1,2} & L_{1,1}U_{1,3}  \\\\\n",
    "        L_{2,1} & L_{2,1}U_{1,2} + L_{2,2} & L_{2,1}U_{1,3} + L_{2,2}U_{2,3} \\\\\n",
    "        L_{3,1} & L_{3,1}U_{1,2} + L_{3,2} & L_{3,1}U_{1,3} + L_{3,2}U_{2,3} + L_{3,3}  \n",
    "     \\end{pmatrix}=\n",
    "     \\begin{pmatrix}\n",
    "        1 & 2 & 4  \\\\\n",
    "        3 & 8 & 14 \\\\\n",
    "        2 & 6 & 13  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "Once you solve the above equation we get the following values:\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        1 & 0 & 0  \\\\\n",
    "        3 & 2 & 0 \\\\\n",
    "        2 & 2 & 3  \n",
    "     \\end{pmatrix}$\n",
    "$U = \\begin{pmatrix} \n",
    "        1 & 2 & 4  \\\\\n",
    "        0 & 1 & 1 \\\\\n",
    "        0 & 0 & 1  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "We can confirm the product of these two matrices would give us the original matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5377db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([[1,0,0],[3,2,0],[2,2,3]])\n",
    "u = np.array([[1,2,4], [0,1,1],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8cd2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [ 3,  8, 14],\n",
       "       [ 2,  6, 13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(l,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a663710",
   "metadata": {},
   "source": [
    "Finally, the last method is <b>Cholesky</b> method where both the diagonals of the upper and lower triangular matrix are the same. \n",
    "\n",
    "We will follow the same example as above and decompose the matrix $A$ into the two matrices. \n",
    "\n",
    "In this case\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        x_{1,1} & 0 & 0  \\\\\n",
    "        L_{2,1} & x_{2,2} & 0 \\\\\n",
    "        L_{3,1} & L_{3,2} & x_{3,3}  \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "$U = \\begin{pmatrix} \n",
    "        x_{1,1} & U_{1,2} & U_{1,3}  \\\\\n",
    "        0 & x_{2,2} & U_{2,3} \\\\\n",
    "        0 & 0 & x_{3,3} \n",
    "     \\end{pmatrix}$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$LU = \\begin{pmatrix} \n",
    "        x_{1,1}^{2} & x_{1,1}U_{1,2} & x_{1,1}U_{1,3}  \\\\\n",
    "        x_{1,1}L_{2,1} & L_{2,1}U_{1,2} + x_{2,2}^{2} & L_{2,1}U_{1,3} + x_{2,2}U_{2,3} \\\\\n",
    "        x_{1,1}L_{3,1} & L_{3,1}U_{1,2} + x_{2,2}L_{3,2} & L_{3,1}U_{1,3} + L_{3,2}U_{2,3} + x_{3,3}^{2}  \n",
    "     \\end{pmatrix}=\n",
    "     \\begin{pmatrix}\n",
    "        1 & 2 & 4  \\\\\n",
    "        3 & 8 & 14 \\\\\n",
    "        2 & 6 & 13  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "Solving this system of equation gives us these two matrices:\n",
    "\n",
    "$L = \\begin{pmatrix} \n",
    "        1 & 0 & 0  \\\\\n",
    "        3 & \\sqrt{2} & 0 \\\\\n",
    "        2 & 2/\\sqrt{2} & \\sqrt{3}  \n",
    "     \\end{pmatrix}$\n",
    "$U = \\begin{pmatrix} \n",
    "        1 & 2 & 4  \\\\\n",
    "        0 & \\sqrt{2} & 2/\\sqrt{2} \\\\\n",
    "        0 & 0 & \\sqrt{3}  \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "We can confirm the product of these two matrices would give us the original matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb486d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array([[1,0,0],[3,np.sqrt(2),0],[2,2/np.sqrt(2),np.sqrt(3)]])\n",
    "u = np.array([[1,2,4], [0,np.sqrt(2),2/np.sqrt(2)],[0,0,np.sqrt(3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "befe33d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  4.],\n",
       "       [ 3.,  8., 14.],\n",
       "       [ 2.,  6., 13.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(l,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea850a",
   "metadata": {},
   "source": [
    "Now instead of calculating the L and U matrices by hand, the `scipy` package provides the `linalg.lu` function that gives you the LU decomposition of a matrix. Note that this functions uses a slightly different method to calculate the LU Decomposition. The `linalg.lu` function uses what is called a PLU decomposition.\n",
    "\n",
    "More information about the scipy function can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html).\n",
    "\n",
    "And more information about how the PLU decomposition works can be found [here](https://johnfoster.pge.utexas.edu/numerical-methods-book/LinearAlgebra_LU.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264ab3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Matrix\n",
      "[[ 1.          0.          0.        ]\n",
      " [ 0.66666667  1.          0.        ]\n",
      " [ 0.33333333 -1.          1.        ]]\n",
      "\n",
      "\n",
      "Upper Matrix\n",
      "[[ 3.          8.         14.        ]\n",
      " [ 0.          0.66666667  3.66666667]\n",
      " [ 0.          0.          3.        ]]\n"
     ]
    }
   ],
   "source": [
    "import scipy \n",
    "import scipy.linalg\n",
    "\n",
    "a = np.array([[1,2,4],[3,8,14],[2,6,13]])\n",
    "\n",
    "p,l,u = scipy.linalg.lu(a)\n",
    "\n",
    "print(\"Lower Matrix\")\n",
    "print(l)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Upper Matrix\")\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082c5e1",
   "metadata": {},
   "source": [
    "Reason why the LU Decomposition is so important is that it gives us an alternate approach to solving linear systems. We know that Gaussian Elimination can be used to solve a linear system of the form $Ax = b$ (more information on Gaussian elimination in my first article). However, the algorithm for using Guassian Elimination takes $O(\\frac{1}{3}n^{3})$ opertations. This is where LU Decomposition can help. \n",
    "\n",
    "Let say we have the linear system $Ax = b$, where the matrix $A$ can be decomposed into a lower and upper triangular matrix. What we get is:\n",
    "\n",
    "$LUx = b$ \n",
    "Let $y = Ux$ \n",
    "\n",
    "We can solve the equation $Ly = b$ using forward substituion in $O(n^{2})$ operations. And then we can solve $Ux = y$ using back substitution in another $O(n^{2})$ steps. \n",
    "\n",
    "This reduces the time complexity to $O(2n^{2})$ steps. For large systems this can reduce the time significantly. $^{2,3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7c040",
   "metadata": {},
   "source": [
    "## Part III: Eigen Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69871e97",
   "metadata": {},
   "source": [
    "The next decomposition is the Eigen Decomposition. To find the eigenvector and eigenvalue of a matrix A, we know there exists a non-zero vector $u \\in \\mathcal{C}^{n}$ for which a scalar $\\lambda$ satisfies the following equations:\n",
    "\n",
    "$Au = \\lambda u$\n",
    "\n",
    "\n",
    "We can put the eigenvectors of the matrix A into a matrix denoted by U, where each column represents an eigenvector of the matrix A. The eigenvalues are stored as a diagonal matrix denoted by $\\Lambda$. \n",
    "We can rewrite the above equation as: \n",
    "\n",
    "$AU = \\Lambda U$\n",
    "\n",
    "Solving for A, we get the equation: $A = U \\Lambda U^{-1}$\n",
    "\n",
    "This gives us the eigendecomposition of the matrix A. \n",
    "\n",
    "**Note not all matrices have eigenvalues\n",
    "\n",
    "Lets solve an example to see how it works:\n",
    "\n",
    "We have a matrix A\n",
    "\n",
    "$\\begin{pmatrix} \n",
    "        1 & -2 \\\\\n",
    "        3 & -4 \n",
    "     \\end{pmatrix}$\n",
    "     \n",
    "From my previous article, we know we can get the eigenvalues and eigenvectors of a 2x2 matrix by solving the characteristic equation \n",
    "\n",
    "$ det(A - \\lambda I) = 0$\n",
    "\n",
    "which, for a 2x2 matrix is equal to:\n",
    "\n",
    "$ \\lambda^{2} - \\lambda Tr(A) + det(A) = 0$\n",
    "\n",
    "Solving the above equation, we get the following eigenvalues:\n",
    "\n",
    "$\\lambda = -1$ & $\\lambda = -2$\n",
    "\n",
    "The eigenvectors for these corresponding eigenvalues are:\n",
    "\n",
    "For $\\lambda = -1$ \n",
    "\n",
    "t$\\begin{pmatrix} \n",
    "  1 \\\\ 1\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "  \n",
    "where t is a nonzero scalar \n",
    "\n",
    "For $\\lambda = -2$ \n",
    "\n",
    "t$\\begin{pmatrix} \n",
    "  2 \\\\ 3\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "  \n",
    "where t is a nonzero scalar \n",
    "\n",
    "So using the notations introduced above our U matrix would be \n",
    "\n",
    "$ \\begin{pmatrix} \n",
    "        1 & 2 \\\\\n",
    "        1 & 3\n",
    "  \\end{pmatrix}$ \n",
    "  \n",
    "and $\\Lambda$ equals \n",
    "\n",
    "$ \\begin{pmatrix} \n",
    "        -1 & 0 \\\\\n",
    "        0 & -2\n",
    "  \\end{pmatrix}$\n",
    "  \n",
    "\n",
    "We can confirm that the product of these matrices would give us our original matrix A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6a6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "U = np.array([[1,2],[1,3]])\n",
    "\n",
    "lam = np.array([[-1,0],[0,-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d30915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e025b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -2.],\n",
       "       [ 3., -4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "A = U @ lam @ inv(U)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f8196",
   "metadata": {},
   "source": [
    "As you can see, this gives us the original matrix A back. \n",
    "\n",
    "We can use the `eig` function from numpy to find the eigenvalues and eigenvectors along with `inv` function to confirm as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a2196f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -2.],\n",
       "       [ 3., -4.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import linalg as lg\n",
    "Eigenvalues, Eigenvectors = lg.eig(np.array([\n",
    "\n",
    "[1, -2],\n",
    "\n",
    "[3, -4]\n",
    "\n",
    "\n",
    "]))\n",
    "\n",
    "Lambda = np.diag(Eigenvalues)\n",
    "\n",
    "\n",
    "Eigenvectors @ Lambda @ lg.inv(Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67675872",
   "metadata": {},
   "source": [
    "Eigendecompostion plays an important role in many machine learning applications. Some machine learning applications include Principal Component Analysis, Spectral Clustering, Computer Vision and many more.<sup>4</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d0866",
   "metadata": {},
   "source": [
    "<sup>1</sup> https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/\n",
    "\n",
    "<sup>2</sup> https://johnfoster.pge.utexas.edu/numerical-methods-book/LinearAlgebra_LU.html\n",
    "\n",
    "<sup>3</sup> https://www.sciencedirect.com/topics/computer-science/gaussian-elimination\n",
    "\n",
    "<sup>4</sup> https://towardsdatascience.com/the-essence-of-eigenvalues-and-eigenvectors-in-machine-learning-f28c4727f56f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77336209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
